 Hello everyone. We're starting right on time because this is the most exciting webinar we've had yet. So this is the only webinar we've started on time so far, but it definitely deserves to be this one. We've got an absolutely packed webinar. Some awesome projects, some awesome people. And so we're going  have a really fun conversation. I'm in hand over to Charles very quickly. I just wanted to say first of all, thank you everyone for joining and being interested and then thank you also to Charles Matt, you'll be joining for rejoining and spending an hour with us chatting about some  play fun topics. And with that, I'll hand it over to Charles. Nice. Yeah. Thanks, Harrison. And just to clarify for everybody asking in the chat, as far as I know Harrison is not operated by a Lang chain agent, he's operated entirely by my logic. I have Dean once in real life, there is a real Harrison cheese.  So, yeah, so with that, let's kick it over to the people quickly making that harder to be a joke. And we're going to hear just five minutes on each of the three projects that we have here. So, Shen, you will be talking about his work on react, the sort of like fundamental prompting and control flow pattern.  that now powers a lot of agents in Langshane and elsewhere. We'll hear about the Baby Agi project from Yohei, which is really sort of like captured people's imagination and sort of demonstrated just how far you can get with even like a relatively simple pieces composed correctly.  And then lastly, we'll hear from Matt on some experiments with co-generated agents. So I know the least about that third project. So I'm excited to hear about that. So yeah, shouldn't you if you could go ahead and think you had some slides you wanted to share about your work on React and  and elsewhere. Oh, thank you, Gary. Yep. Cool. Yeah, so I just have to go through the quick slides you share. I'll do it in multi-weight, you know.  what they mean to do the react. So I was doing this project last year in the summer and at the time, you know, it's like Palm just got reduced. It's very much been there for a while. Everybody's excited. Channel saw just released. And the observation that I make is just, you know,  Long small, so getting that that here has two out of the fundamental capabilities for task software, reasoning and acting, but in a rather separate manner. So if you look at some of the reasoning work, like she have thought, it's really generating those self-conditioned rhythmic traces to help solve many different tasks, but it's not really grounded to any excellent environments. And on the other hand,  At the time, some exciting work, including CCan, shows you can collect them for the two optional environments, two external works, by using this kind of grounding. But they're not really using the reasoning capability, or generally having a business interest. So the natural question that I ask is, can you combine both and like the mode of  and both with interest and actions. And that's basically what you do react. And I think the easiest way to think about this is that you have an agent and you want to do all kinds of autonomous tasks. And personally, you know, people do this kind of observation, action, action, kind of iteration.  But the idea is very simple, right? So we just add a new action into action space and this action is called sinking. And the sink can be any kind of thought that you have in your mind when you try to map from observation to action. And it's good because first it gives you like  computation that you can use south and as per your question, think about what you want to do and do this non-schirer mapping. And then it helps you to do the left better because like based on the reasoning you can generalize to a very different scenario, maybe it's sugar sour, then you want to turn left. And finally it's really a great alignment measure because you can directly  I'll just stand wide, hard, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up, up,  a way to algorithm the reason is because we know something that chat to you is terrible as some of the foodization problem because even the largest model in the world can out like exhaust all the knowledge in the world. So there could be a witness and there could be a native knowledge and you have to enter out with environments like Wikipedia or Google or external launch basis to really  can a chance to argue me out of the day of the college. I just want to quickly share what the original demo looks like. It's a very poor,  a jibis and notebook compared to the fancy mansion, but after all this kind of script like that, and it's a good use of what is a bank model with opening a adding, maybe a new development environment and environment is kind of like opening an item where you have a step and give you an observation, and the prompt looks as  like this, right? So you have like an instruction and you have some trajectories of you know what kind of human way of solving the task looks like and you can have a few shot trajectory and then you just do this kind of like sort action observation loop and  So give a very quick example. So here's a question right. So how many chemiships have career volume? So let's send a brown james. And the way the rest of this, you know, it's like the search is on Wikipedia. And then it tries to, you know, just listening to it's trying information and then plan was in the collection.  And this is off the question of end. And I think when create comment this, you know, if you look at something like something like a child sobriety, it's not really able to sob it. So if you give it a try. Uh, show you, if you could just zoom  a little bit, some folks on lower quality connections, I think are having trouble seeing some of the text. Let me try my best. Yeah, you do. Yes. So I just want to show this invitation of Chao Sao, which is, you know, because it doesn't  As a real know, Steph Curry has another championship in 2022. So he thinks Steph Curry has three championships and the bra has four. So Steph has less championship wins the bra and James and that's a typical example of like if you're Michelle did it. And yeah, I think I'll stop right here.  like the thought action, thought action observation, kind of like sequences use a lot of places now, including in LinkedIn, very heavily. How do you come up with that kind of prompt engineering flow and those keywords and did that require a lot of experimentation, or did you try to try that one to kind of work?  I think it's more of the lack of situation. So I think maybe there could be better from generating to making work better. So once seeing I still have a figure out, it's like, it's a better to have those strict, you know, sought action and observation loops. Or it's a better to just put saw this like a special action called sinking action.  the reasoning being, you know, if you look at that mention about demos, many of the time you don't really need thoughts in some of the intermediate actions. Maybe it needs not only sparkly. So that's the thing that happened to your girl. Some of you values in my behavior. Yeah, I mean, so that's another question. How did you kind of evaluate  this and I'm assuming it should just came out as a paper and so I'm assuming there were some quantitative metrics and so yeah what are those metrics do you think they're good and and are there any others that you're excited about? That's a great question so you fact I think the hardest part about my paper is try to come up with tasks to your value  because if you look at the traditional NLP task, they're not really meant for this kind of autonomous interaction kind of thing. So what people typically do is that for something like faster answering, where you have a very, especially trained retriever to overfade to that task, and then you have like a answering module specifically fine to you that  task. So it's becoming like a lumbering game. Also, I think, on the other hand, you have those RL tasks that is very hard to prompt things with RL. So the trade off I chose to risk an task and to RL  house and our old house are kind of like a text game kind of version, except real robotics or video game. And for the reason task, it's like a question and three task and a factularification task. I think it's surprising to say that on the our old scene, it's really, really  great performance. So our performance is better than where are you saying only white shouted to a shop prompting? Or the performance is much better than you know, in addition to learning stuff are using like 100,000 examples. So at a time I know there is something about this. Yeah, so there's lots  lots of questions coming in that we could cover on React, but I want to make sure that we get to see all the projects before we dive into debon. And there's lots of consonants and connections to make. So I think we'll kick it over to Yo Hey for just a couple of minutes to talk about what's up with the baby ADI. Hey, I think this is actually the first time I'm  like talking about it. Like it's all tweaks. So if you follow me, you've probably heard this, but I'll just do a quick story is that I was looking at the whole Genesis, the baby age, I was looking at the hustle GPT project where people were using chat GPT as their co-founder and I was fascinated by it and I wanted to do it, but I didn't have the time. So I asked myself, can I cut the human out of  bit. So what can I just make an AI founder start a company for me? And I made a prototype on GPT4 right, having it right all the code, shared a prototype. Really, I just said this is, you know, like, my AI founder is kind of working. And then my friend Jenny was like, oh, dude, just make baby AGI. And then that's just kind of caught on and it's just like everyone started  calling and that's what I was like, all right, well, people are excited about this. Let's do a research paper. Apologies, it's not a real research paper. I simply fed the code to GPT4 and said, please write me a scientific research paper. And that was the up. So it's absolutely not an actual scientific research paper, but it seemed in line with what I was seeing in the market. And then three  I thought, well, this is out. Might as well open source my code. I like simple code, so I really paired it down to what 105 lines of code. I took the original nickname that Jenny gave it, and released it on GitHub.  So let's let's back story all like I can explain how it works, but the way it works is you know it's it's actually the same way I work um which is that uh you have a task execution agent that just does the first task on the task list then at the end of a task there's a task creation agent who says if you finished the task what other tasks should we do based on that results  And then a task prioritization agent who basically reorders the tasks and sends the first task to the task execution agent. And I designed it that way because that's exactly how I work, right? I start the morning. I jump on my to-do list. I start on the first one and I just execute. If a new task comes up, I add it to my to-do list and I just keep executing at the end of the day, I go through  my task list and I reprioritize the next morning I can wake up and I just start at the top. Again, I know that's very systemic in terms of the way I work, but I literally just map that into the Asian. Yeah, I think one of the things that immediately like jumps out looking at the generative agents work that  just came out from Google and Stanford. And at Baby Agi and others is this problem of prioritization. And how do you interleave meta thinking about which tasks you should complete with just the actual completion of tasks? So it's like quick cursory over look of Baby Agi. It looked like the primary thing  was this like task you for management, but it sounds like mentioned to sort of like before bedtime reprioritized kind of thing. So how does that sound like? Yeah, I mean for me, you know, as a human, right? I don't have the same cycles that that baby age I does. So my task prioritization agent only works at the end of my work. But that being said, I'm sure I do it sometimes, right? Like if I  I get an email from an important limited partner. I might respond to that email before I do something on my task list. So I think that that goes into where do you create an input? Can you listen for incoming emails or messages and then create tasks based on that, which was an implemented but was always part of the initial diagram?  The part that jumped out about baby agi to me compared to the kind of like react style prompting is like it is exactly this task prioritization list. And my sense is that this probably helps with like more complex tasks where you want to like plan it up front and then kind of like offload stuff as opposed to maybe it's like simpler things like even two hop things you can  and kind of just do it on the fly. Is that the right way to think about it? Have you noticed any like, have you compared the two approaches in any kind of way and, and, and, and, yeah, notice different seasons in terms of where they're good, where they're bad? I think what you said is right, right? I think with with more complex tasks, having a task this ahead of time, probably is more  It makes more sense than just guessing one by one, because you do need to plan. In at least in this case, the objectives for BBAG, initially when I said it, there was no finit and it was like, it was like, a gruel or company, right? So by design, there was no finit and which means  that you have to keep creating tasks, right? As a founder, there's always things to do. So that's a by design that's different. But as people start using it to create objectives, it does seem like a lot of the experiments are around beefing up the task list. So you can kind of do serialization or sub-task, or milestones. But I do think that's where a lot of people are.  I wanted to pull one question from the Q&A before we kick it over to Matt here about the co-generation work because it felt quite relevant to what you and Harrison were discussing which is like what are the kinds of tasks that you've seen the baby AGI actually fails on  or struggles with and maybe some suggestions of where things might improve. I think people see cool Twitter demos all the time which are generally the things that work but there's this sort of negative results problem. So it's harder to know where are these things failing and that's where the good ideas for what to do next come from. So any thoughts? The first thought about  The problem is, it's not good at doing the things that people want it to do. It's really early, right? It's just a framework and idea. People immediately jump to what it can do. It's far from the first thing that people imagine. There's a lot of work in getting the tools to work together and all that.  This might be a kind of a cop out answer, but what it's really good at is getting people excited about building. I think it's indigated a lot of people, a lot of people who weren't building, jumped into build, people who weren't sure what to build next, jumped into build after seeing these projects. So again, that's not really the problem that the code solves.  but that's my observation of what it's been where it's really succeeded. But on the specific tactical, right, I think with a, I have one BBAG version with a link chain that the most complex task it did was I asked it to create a folder in my Google Drive with a document describing  each of the main friends characters. And it was able to do a web search, create a Google Drive, drop six documents in it, and then add one bio to one of the six documents. And then it cropped out. So, you know, good and there. Not bad. Yeah. I've definitely worked with people who could do less.  All right, so thanks for that and we'll kick it over to Matt to hear a little bit about what he's been working on. You can give a better introduction to your project than I can so go ahead and take it. Yeah. So I think the GPT for in particular is the reason why we're seeing this explosion and like  the GPT4 and your hey combined to create this ability for the rest of us to imagine what's now possible is really how I'm doing this moment. The step function between 3.5 and 4 is so huge.  I mean, not just going from 4K to 32K, but it actually like pays attention to almost all of the context. Like everybody remembers GPT 3. If you step 4K, you're probably doing something wrong because it was never going to do what you wanted it to do. And you know, don't get me wrong. 32K GPT 4. If you try to step 32.  K tokens will also start to get a little bit confused too, but now if you stuff 10, 15,000 tokens, it really does pay attention. The second thing and I'm not sure whether the existing models have really fully taken advantage of this yet is that it follows the system prompt for  instruction following incredibly well. In fact, I've heard it like almost described as it follows it almost too well. Okay, so what does that allow us to do? That allows us to actually define a protocol that we can have a reasonable expectation of GPT4 to follow what  And we're essentially embodying it with the capability to take in inputs and then perform actions in the real world. So the project that I want to share is very, very simple. And it's kind of a thousand feet back up from what Yohei has done. And I think  You know, the thing that I really want to come out of this is, you know, hey, like you're a project in my project need to meet in the middle somehow so that we can have, you know, some killer, uh, uh, uh, BABHA I code agents. Um, and, and that's a project that I'm just calling Yamil runner and, uh, Yamil runner consists of, you know, little  applications that runs these, these EAML scripts that are defined in a protocol that you put into the system prompt. And I can just kind of show you guys what I mean if I can share my screen here.  In the meantime, you can see next. There's a Q&A button right next to the chat. She'll be on the right-hand side.  screen, you can put questions there and vote on questions that you want to hear the speakers discuss. And then Matt, you might want to zoom in a good bit on that one. A little hard to see.  I think Matt's lane chain agent. Cut that. Broke. Yeah. We were at 32K context limit. That's my guess. Yeah. That's what it was. He's trying to try to stuff it too much. Yeah.  Well, we see what's happening there. Let's see. There's a question. Hi, I have a question from the Q&A that I'd love to hear you. Hey, and should you talk about like we're starting to see sort of like patterns of how agents work both at the level of like, you know, low level prompting techniques about like  thought observation action and then also control flow techniques and augmentation techniques. So like have a memory stream that you can access that's like stored in a stream oriented database or you know these like control flow things around when when to recursively improve and when to like  like when to stop and act. So I'm curious, what kinds of patterns are merging that you'd seen that are maybe surprising or people might not have heard of? And of course, Colin wants to know who's going to write a book on this so that we can all just buy the O'Reilly book and know what the good patterns are? I think, I mean, I think that's what's  is that right with with VVHV, is you having so simple people are taking it a lot of different directions. So some people are working on having agents talk to like having the task execution agents talk to each other or having two of the VVHIs talk to each other or separating tasks into sub tasks and then spin up spinning up another chain that's just handles that sub tasks.  and reports back to the main. And just like more task manager, just more complicated organizational structures. I've seen so many, but those are some of the ones that popped mind. In terms of the I just seeing more Asian patterns.  I think you're muted. I think for example, you know, self-referment or self-critic thing, I think it's quite exciting. So you can actually look at what you have done and like reflect, you know, what you have done wrong, improve it.  I think it's almost filled like, you know, in the traditional aisle framework, there's a lot of like division modules where you have to credit, you have like a planner, you have a controller and stuff. And it seems like we can already start to replace everything with a different large amount of model and then start to build very  do you think like maybe is there a limit to how good you can get with just self criticism like it feels a bit like cheating. I know I just see a lot of successful results before I really believe that self criticism could work as well as it does. And now I think  I think obviously there's an image, but I'm not sure if we have a rich tag yet. In a sense that people are eager to publish whatever is you could publish first and then try to give them a size of an idea. So I don't think people have a  carefully examine all the possibilities and explore the media. But like everything you mentioned, I feel like we still have an explore the limitation of something yet. So I'm really excited about this. We've been thinking about this a bit at LaneChain as well, because obviously with all the new types of agents coming in and we've been trying  It's like what kind of like the differences are between the React style stuff that we have now and what are the new pieces and what are missing. And I think from. And so I think we'll probably write something about this because I think this is a really good question that everyone's kind of what we want to know. And I think like. I think in addition to the React style prompty, the new things that.  I've seen, I think the baby age I introduced is this like planning and execution kind of like step and separates that and I think that's something novel and I think that's something new and I think that unlocks some like new longer running things and then I think also compared to again like the React style thing is all kind of like in buffer memory and that kind of like  runs out of space when you get longer running things, which baby age you know, I think it's also different tasks that they were designed to do. I think the reality style prompting was designed for a lot of these multi-hop kind of like two-hop question answerings, the baby age you guys designed to like, you know, solve world hunger or something like that. And so they of course they have different kind of like memory constraints.  and how you construct the buffer. And then the third thing that I think, and this kind of combines some of the, this kind of gets to like the planning execution, but just more generally like, yeah, the idea of, I forget who said it on Twitter about like stacking of language models and just suffering it out like, you do this task, you do this task, that's fine and not have one big thing. And so I think those are three  kind of like recent trends that I've seen between. I think Baby I GI does a lot of those. The auto GPT is the other kind of like big one. I think that's mostly just like taking advantage of GP4 plus this long term memory part. I think they add in that kind of like unique and novel long term memory samples that the vector short  But yeah, echoing kind of like what you always said it's yeah, there's a lot of there's a lot of cool stuff going on in this it's just the beginning so whoever writes a book now will have to be updated in about a week. Yeah, it looks like we got Matt back if you want to maybe do  see if you can share your screen again. We'll give it one more try. I'm more sure. Sorry, I live in upstate New York. My internet connection was like a third world country. All right, here we go.  Okay, so the basic idea here. I'm not. Good bit more there because like yeah, like maybe 200% or something and there's this good enough more.  Yeah, I'm going to say a lot more actually. Yeah. Okay. Yeah. Got it. Yeah, so the debate I keep this super simple so that we can get back to you know the general discussion but the view that I have is that we essentially  actually, you know, the really practical thing right now is what I'm just calling pseudo agents, which don't do quite as much on their own as, you know, we're asking baby GPT or on GPT to do, but really allow us to work effectively, offload a bunch of work to  the large language model, but really be part of the loop in the decision making process. I think that there's going to be a ton of UX that goes into this in the next couple of months, but of course, I think it's a time we make a time prediction. It gets beat by half these days.  And so that the basic idea is that we give it this protocol, which I have an example of called Yamonroner, which gives a very, in instructions to follow, as to how to take directives from the user and then turn it  into a sequence of YAML runner instructions. And so here's how I'm using this. So first of all, you want to put in as much context as you can for code, at least up until the point where you don't want to confuse GPT for run into context window limitations.  And after that, you can see that you just ask it to ignore and understand that it's a reddit. And then I just did a really quick example, which is, you know, what's the easiest feature that you could add? And it decided that the easiest feature that it could add would be to  to display a version. And so it creates this Yamal Runner file. And that includes a couple of very, very simple operations. And the Yamal Runner can execute commands. It can edit files. It can add files to file operations.  like a lot of the GPT agents. And we can generate a new sequence of instructions and we can execute this and it will do all of these things. And I think that what's really interesting now is that the GPT4 has clearly been trained to do  co-generation problem solving within code and trouble shooting within code. So that actually lends it very naturally to the use of GPT4 as a co-generation agent. The trick though that I've found is that you really need to treat it like a dance. It really needs to be  something where sometimes GPT-4 is leading the diets and sometimes you're leading the diets. And that's why I don't think this can be quite automated to the extent that we can fire and forget baby GPT and expect that to work and that's where we expect the UX to really come in and fill the gap to make fast iteration possible.  possible because it's really is truly enabled by GPT4 now. So Matt, you, you, when did you start working on some of this  stuff. So I know we chatted for the first time back in October. November something like that. And so I guess like, yeah, when did you start working on this and you've touched a bunch on, yeah, how GP4 is enabling a lot of this now, but like, you know, lower the biggest differences between working on it back then and now and how it  Yeah, and does that hold any kind of like clues for what it might look like in the future? And I also think we're zoomed in on an empty stage. So I will try to fix that. Yeah, so I started playing around with GPT 3D, DaVinci, W2, code agents in the summer last year.  And why I found is that it wasn't very useful yet for a number of reasons and I'm sure you know a lot of people, everybody in this chat and a lot of people, you know, watching ran into the same sorts of things where I don't want to get to many of the same problems now only we're just all free but to  solve more sophisticated and complex problems. But cognosis shift with a module called Ijavascript, which was a GPT-3 agent that was embodied using a node repel that was put in a loop that did automatic summarization of the various steps that had  taken. It had to also frequently remind DVNC she does what it was doing and what it lasted because it would very easily lose track of that. The bigger and larger models have certainly just expanded the level of complexity that they can deal with. I think that the real challenge  to day remains in two different categories. The first is context, of course, and context management, not just context length, but context management, where even if you have 32K or where we get 640K, you know, next year,  here, hopefully, who knows. And we're still going to run into the limitations of what we want to be of putting members. So we're still going to have to do context switching and essentially do, you know, with the virtual memory machine does in an operating system, which is swap pages in and out depending on, you know, what the process  needs. The only problem that we have that kind of extends past that is there's a deterministic way of knowing what pages to swap in and swap out from the operating systems perspective because the process knows exactly what it needs to and and and ask very specifically.  for you know that the files or pieces of memory that it's looking for. So we actually have to come up with some kind of query planner. I think to really solve this problem, I think we're a long way away from solving this problem. Again, along the way from solving this problem in 2023, who knows what that means it might mean next week. I think the second problem  we have to solve a game is a lot on the UX side where these things really aren't going to be able to provide a lot of useful work until we get the UX dialed in so that we have this tango between the agent and the user and my aim with this project is  just to create some kind of protocol that can be used to play with different, it baby AGI style projects on one side and play with different UXs on the other side so that we can create an embodiment standard essentially that we can really kind of  figure out what that UX looks like and really lean into the fact that, yeah, of course it's not going to be a fire and forget, but humans aren't fire and forget. It's not like you can have a software engineer working on a project and just say, you know, one sentence, three and go, go, do it, go go.  future. Anything of any complex in you at all, it's going to be a dialogue. So I'm really interested in that because I think everyone's a lot of people are thinking about the right UX for these things, not just agents, but just generative in the AI in general, and I think the main one that's emerged is chat. And so like, is chat a good UX for this?  and baby agi specifically is not really a chat thing. And so like yeah, how do you guess think about that? I mean, I can speak for myself, right? I think the biggest challenge with chat GPT or chat interface is that it only works as long as you're there. And what I want is for stuff to get done  when I'm not. And so the user interface that I want out of my autonomous tooling is pretty simple. Almost like an API probably, right? I want to be able to flag a task list for my task manager and just say, let's send this to my age.  BBAGI and then just have it kick-in-do-ass. But going back to how far away are we from things being used, so the most earlier use cases will be non-generic, like a very specifically tailored autonomous agents where you craft a prompt for a specific use case. That's going to be pretty quick from where we are. I think turning BBA is into something  that can do all of the tasks from any prompt is going to be a lot more work. But I think in the short term, building out these very simple straightforward autonomous agents and plugging it into your workflow seems like something I see myself doing in the next few weeks.  research side to an extent you're like thinking about or participating in conversation about UI and UX. I've noticed there has been some really great work coming out of Google Research going all the way back to like AI chains and and prompt chainer like on this kind of stuff. So I'm curious if you've thought about that, have anything to contribute from the more research you saw.  Yeah, just want to continue on your head's point. I think like to me there are just two different types of interaction. So what is kind of automatic and what is interaction is human. So if you think about, you know, interaction with code execution or interaction with web pages or digital environments.  etc. Are you looking like with another agent versus you know, by any time, it's a human throw d'Ala. That's in the fundamental difference they said. For the law, for the former cases, to give the feedback in automatic manner versus if you're into a human you have to wait until the human reply and taping the same for you until you get that human feedback.  And in some sense, I feel like human people are like a WHO's word in a sense that that's probably the strongest alignment measure that we have. No matter is RLHF or you know, aligning the model with us throughout our love. That's a biggest alignment measure for us. But that's also very hard for the development  the person researchers or if you really want to do something you cannot afford to be someone like opening an eye and do our LHF and everything. So that's actually the motivation for another F-9 work called upshop and the idea is that how can you feel like a complete task not just you know something open in it like baby age  But you have to get a extraction, you have an environment. But most importantly, you have to have a way to evaluate the bottom and give that a bit back to the model automatically. And I guess from the research perspective, we need to push the frontier of how a ton of us the agents can do.  be but on the other hand we also need to synergize those autonomous agents and plug it into our Refugement Interaction and I see you know there's no reason not to do both. I actually one question. I was going to add one more thing on top of it if you don't mind. On the UI  I do want to set it up for a get it by agree that as tasks get more complicated like I do want to be in the loop and it was a super lightweight demo. I think I use Zapier NLA and I actually used Zapier's text but I crafted the title to be like text boss if you're not sure of answer. So what happens is if it doesn't know the answer or if it has like a list of five in the answer to  one, it actually sends me a text message asking for it. And the goal would be to have a listener in the task creation agent so that whenever I do respond it'll then continue with the task. I think is the UI that is ideal for me because if I have a ton of this agent so I'm going to be able to just text back and forth. And so you did you explicitly add that as kind of like a you explicitly  Yeah, that has a tool or do you kind of like tell it if none of these tools work then do this. I created a tool called, you know, ask the boss if you're unsure about decisions. And I don't think it's the right. I don't think it was the right one, but it works well enough that I think it's worth, you know, if you can if you tweak it, it can get better. Yeah, that's so much  What's up? The E to B project is something very similar, I think, where it has, it has a couple of things that it will expose as tools. One is to prompt the user and the other is to add  ask it to make a decision, which is kind of neat. I haven't seen it actually in action, but I think that's probably the kind of interactions that we're going to need to have with it to your point. And on that one, one interesting thing that I saw, like completely anecdotally when working  with some agents and line chain is that like when you give it explicitly a tool to like ask it does much better than if you're like if you don't if you can't use any of these tools like then respond directly to the user like if you give it explicitly a tool it's just much better for whatever reason and so I think yeah whether that's a tax or like an input  and if you're in like a Jupiter notebook or something else, I've seen that basically be the best strategy for getting that to the point. I do want to bring in one of the questions from the Q&A that had a ton of up votes in his relevant here, which is in particular, we've been talking a little bit  Maybe about a single system, primarily here, whether it's dialogue or an autonomous system. And one of the questions in the chat was about developing and managing large numbers of agents, maybe a bunch of tasks specific individual, baby agi, who only care about one particular thing. So do you think that that things change when you  scale it up to like 10 or 100 autonomous agents. What does that look like? Does it look like a Slack channel? Does it look like an MMO video game? Does it look like your email inbox? What what a folks think? And there's that there's that paper recently that came out I think like Monday or something like the simulocre.  which seems very related to this. Have any of you guys read that and detail or about that? That paper is pretty wild. Including the, so this, this, this, a gender vagins paper. It's from the Center for Research and Foundation models and Google Research. They created like a star do  Valley type system where everything was controlled by GPT 3.5 turbo and they were able to like at half or a quarter real time simulate like 25 agents who had like a memory the ability to reflect on that memory and adjust their personality over time and to interact with this simulated environment.  And a lot of the pieces, the pieces are all very familiar. They look like reasoning observing, then reasoning and then acting. They look like a memory stream that you can read read from and write to. They look a lot like, in fact, they use JSON and like use JSON as a communication interface. So they have pieces, there are pieces  that we see reflected in each of the three projects here in that one. And it does seem like if you made an entire little StarDue Valley town whose job was to help run your startup that does seem like a potentially very good interface for a group of baby agis.  So something that you guys may or may not have also seen is the work that Merith Rosas group is doing on this. They've been experimenting with LLM based NPCs for a number actually I think since last year.  And I saw an early demonstration of it last year and I was like, hey, I'll get that worked pretty well, but like the they're they think they just released something this week or last week that looks really, really good and and represents an advance that I think it's consistent with the new types of models that we're seeing to check that stuff up to.  Another paper that, and I'm not sure which do this is, but there's like the camel paper that someone implemented in LinkedIn yesterday, I think, which is basically just it's just two agents, but it's talking to each other and they chat kind of like their own personalities and their own objectives and you can kind of see the conversation kind of like flowing and I think it was more kind of like  mental than anything else. Yeah, I mean, it seems like there's kind of like highways every two to four weeks. And I think like, you know, the past two to four weeks have definitely been about the B.A.G.I. and auto-GPT. And so I wonder if like the next two to four will be around like, yeah, multiple agents talking to each other. And I think there's a few different varieties that one is like kind of like the  simulation style things. But then I do think there is another variety which is just this like stacking style thing where there's a more controlled flow and there's kind of like a router which is using them as different. And I see them slightly different. Right. Like one's kind of like simulating a real world situation and then the others just like I don't know trying to do a task and the tools it has or just have  to be other agents themselves. And so I'd maybe draw like a little distinction there, but yeah, still so really, really fascinating to see what comes out. I actually like the question about the front end though, because I do, I mean, cost is important, right? Like as you're running, if I'm running a whole bunch of pages to run a company, like cost is going to pile up where it goes  like I would definitely want like a simple log of all the stuff they're doing. Again, I'm coming from the, if I'm running a company and having a whole bunch of agents trying it like for me, I want like a gust, all these are, you know, AI employees. I want to know how much I'm paying, I'm going to know how much work they're doing and I want to know if they need to, if they need to attention.  Yeah, this is another question that we got for the Q&A from David Lou. Like humans, we have, you know, we know that we have limited resources. We're trained how to like use those resources effectively our time, our energy, our money. And one thing that language models don't currently have is any notion of those.  limitations. They have learned our patterns of thought, our patterns of tool use. And so, only text the boss when you're unsure is maybe something they can pick up quickly. But that's not the same as their actual resource constraints. So the question is, how do we get them to behave according to the constraints that are actually an operation for them?  I want you to quickly mention, like this thing has been considered in causing the science, like, meta reasoning, this is that I can spend 10 minutes thinking about this problem, or I can then allow us to give about this problem, but I need to consider the  constraints and it's pretty outcomes and do this for our computations. So if I would have a fly-second then I did you have a very different strategy than I have now and it's really like we are a doke because in some sense we are doing this better recently the same pretty well and it's still  That's very easy. Maybe now it's time to consider this problem. You can have a matter of reason or say, if I only have this kind of money and I know it's talking about this money, come up with a prompt that it's less  I mean, there's a comment in the chat that says like add a budget keeping agent and it's kind of a joke, but kind of serious. And I think this comes back to like, whenever people ask like, you know, how do I get a language model to do this? You have to tell it. Like it's not going to magically  things. And so you just tell it that it has kind of an, and it's, you know, it will be some like prompted scenarios to figure out what exactly you have to tell it and blah, blah, blah, blah, blah, blah. But you know, just have a budget keeping agent. You tell it has this budget, it's a side. So I don't know if that seems like a decently reasonable approach to me. Yeah, the primary agent pattern is literally just  ask bro. I want to take the minutes. No, you go first. I want to jump back on this question actually. I want to take a different direction. Oh, you go first. Yeah, I just put a button on it then. That's  look an incredible like the next step is to start to add like either time or energy or cost-based or token-based constraints. The only thing is that a lot of these questions, a John Carback in his five and a half hour Lex Friedman interview said that he thought through a five ideas you could write on a Mac and that that would probably get  I want John Carmack in April of 2023 to update that. The second thing that he said was interesting and true is that a lot of these ideas have already been created and explored academically.  incredible agent work done even in the sixties. But especially in the mid and late 80s, there are three architectures which are really important to note here. Atlantis, BB-1 and SOAR. And there are others as well. But my suspicion is that something else that we could do  to push this along a lot more quickly is to go back and see, okay, well, a lot of this thinking has already been done for us, but they didn't have any engineering to be able to implement this in a lot of science implement this. Okay, what do we have? It looks like we can now view these things in 2023. What in Atlantis, BD1, SOAR and similar  types of architecture, cognitive architectures, you know, can we learn from it and can we start to solve some of our, or to many steps or problems. Yeah, it's amazing seeing some of these research papers like I hadn't seen many of them, but I've seen a lot recently and it's  It's amazing that these people thought of these ideas so long ago and were in the age that they can be built and it's pretty fascinating. That's why it's called research. It's just such a thing. On one thing I wanted to talk about the question right under the resource contained can the baby age you are these limited? Absolutely right you can just tell again  and tell it to like, hear your constraints, can you prioritize? When I saw the question, I was actually thinking, can I get it to prioritize my tasks? And I think what's interesting about that is that if you wanted to build an, it wouldn't be baby age, right? I'm just saying, can you build a task prioritization that can prioritize your tasks? I think that thought exercise would help you build  build a better task management agent for the AGI. Because you think about what are your constraints, what is the context that you need to feed into your task presentation to get it to prioritize task accurately for you. And if you go through your personal actually like, oh, I need long term memory. I need an overall summary of what I'm doing. I need like a current milestone.  As you think about like, what would all go into an AI, a prompt, maybe, you know, a big prompt that could successfully prioritize my task list. I think if you ask that question and then step back further up, you could kind of generally build a much better task prioritization agent for something like baby AGI. And the reason I bring that up is I think there's a lot of relationship between the way  we operate the way we think our brains and as we think about building these autonomous agents, there's a lot of cross learning that we should absolutely be talking about. I did want to bring one last major topic in our 10 minutes that we have remaining here, which was the question of like  safety. A lot of people are asking about security, like narrowly considered, which is like prompt, you know, is this thing going to suit our MRF? My machine is this thing going to like drain my PayPal account. And then there's also the like broader question of, you know, what happens when we unleash a whole bunch of like much more capable,  like web scraping and API hitting bots. So I'd love to hear just like from each sort of panel member in like what they think the path is for what for you know, us, us waging those concerns that people have. So I'd like to start with Shen you. Yeah.  Actually, I'm writing something about this. Apparently, you know, when I talk to a lot of people, there seems to be about the biggest issues because, you know, in the era of Lentimolo, the biggest harm you can do in this generally like a hateful speech, biased speech, those Asian speech. But now, you can, you know, generate a virus.  really for the new film, whatever. And you say that a hacker can do it, potentially, that the city can't serious much more. And I don't have like a complete solid yet, but I think maybe there are two things that we need to consider. So why is we might need to guarantee you know the worst case scenario, you know, says that.  When you decide to access risk very carefully so that we could have some guarantee of the worst case scenario. For example, in those web navigation, you live with access risk to only post actions so that it's not going to write down the web pages or in Python, OpenNN has started to do this.  some of the OS packages. So I think that's the way to guarantee worst case scenario. In case of average case or if that case, I think the problem is that humans cannot oversee those things like every moment, every time.  this research line of scalable or outside is pretty interesting. And I guess the core idea there is, you know, where you visually need to use agents to, or they say other agents. And then it becomes like a dedication of trust, right? Because like I give this much trust to this.  over seeing agents and then it's over seeing as you will assign some trust value to some of the other agent and it could be a chain. I think it's a very interesting saying and a very important incentive you started and it's a lot of exciting to use. Yeah.  So, Yohei, if you want to give your answer. I think in the short run, right? It's still early. It's definitely not consumer ready to extend. Oftentimes you'll see technologies pretty rough that definitely can, you know, delete stuff from your computer. We see that's all the time. It's not just AI.  But by the time it reaches the average consumer, there's been a nice UI wrapped around it with safety features implemented. I do think the number of non-consumers playing with it is probably higher now. I think due to just code being easier to do. I mean, I'm a good example of someone who doesn't really know his way around the terminal, but I'm just copy  pasting things from GPT, you know, from GPT 4 and like just messing up my virtual environment and I have no idea what I'm doing. So I do think there are challenges like that. That being said, I don't think that somebody deleting files from the computer is not great, but it's also not like world ending by any means, right? As people started driving cars,  as we added more and more safety features and safety rules. Driving cars becomes eventually safer. Now whether the fact that we drive cars did it destroy cities, did it like did good car culture destroy us already? Yes or no? That's that's an unclear question. I think it will autonomous agents destroy you know destroy culture. I assume it'll be a similar  discussion in the midterm and then in the long run like nothing lasts forever right? I hope that nothing lasts forever refers to autonomous agents or our current conceptions of the world not like the human species but I mean  I mean the universe isn't gonna last forever, right? So when I first started experimenting with these things, I was really really  paranoid. And the I JavaScript module in Cognosis which you know it's an artifact today. I wouldn't recommend using it, but yeah, you can look at it to see a little snapshot in time. And I did what I could to  As you pointed out to kind of constrain the actions surface of the action space that that it could operate in by creating a separate repel that you know I actually talked to via sockets so it actually wasn't even operating in the same process as as the rest of the application.  I think that that's probably where this needs to go by default and you know there are different ways that you can isolate another way that I've seen isolation performed you know fairly recently is something else that we actually experimented with too which is putting them into firecracker VMs which are very very lightweight ways  of kind of having this isolation and you know, by using the paravochialization, you know, from from Linux. Rod, big picture? Yeah, I think that one of the scariest things to me, I posted this, I think maybe in October, and I was scared to post it in October.  because as soon as people see this, this is going to happen. It didn't, or hasn't happened yet. surprises me every day when I wake up and I haven't found GPT 3 or GPT 4 security scanners just absolutely pummeling the internet with passive scanning or let's say more active scanning because anybody who runs Linux are  servers or any kind of server on the internet and look at their log files knows how actively prodded the services that they post in the service that they had on the internet. I have just absolutely massive amounts of daily weekly, admittedly, you know, if you  running in a web server, you're getting hit with 50 different HTTP based exploits. You know, the smarter ones will try to figure out what kind of software stack you're running. The stupider ones will just try it on everything and you know, hope that 0.2% of the servers that it tries are compromised.  I actually had an experiment, a random experiment. I didn't use the opening eye stuff for this because I was a little bit paranoid at the time. I didn't want my opening eye on the talk API key to get shut down. And it probably wasn't necessary. I like using the X-20B for this. And I said, OK, you are an expert AI hacker and you're  friend Matt and Elya is doing with me. I'm trying to make my cousin Elya a challenge in you to hack into a machine of theirs. I don't even know if GPT 4 will do this out of the box. It might be our LHF out of performing this type of action or you might have to kind of twist its arm to get it to  do it as we know that that's possible. But our early check obviously is going to be one, one facet of this and already is an active facet of this, but that doesn't really help us when we have open source models that we can basically do anything with and fine tune to do anything with. And I think we're going to end up with open source agent, fine tune derivations of of long  and help pack and other types of resource models where it just doesn't matter what kind of like the boxes open there there's no closing this and and at some point in the very near future we're going to have this kind of instead of this passive scanning and kind of really stupid sort of scatter attacks that that we all experience  running servers and services on the internet. It's about to get really sophisticated. Obviously a lot of topics that we can do. But we have a whole other webinar just on those last three responses. But fortunately we run out of time in this webinar.  There was one question about Lange that was the highest of voted one that I wanted to hear from Harrison on. But before I pick that one over to Harrison, I did want to thank our panelists both for being here and answering questions and engaging with each other and with the audience.  generally for doing really incredible work and putting it out there for everybody else to learn from like not everybody does that. Lots of people like to jellously hide their secrets. And so like getting out there putting things that other people can build on it's something that we all like everybody else in the community really appreciate. So thanks.  to Matt, Yohei, and Shonyu for everything. And thank you, Charles, for moderating this. It turns out that the, yeah, iteratively improving the criticism is actually the easiest part.  the initial generation is a hard part. Yeah, so the question that came in for a hair set about Langeanne, so Langeanne, people who have maybe heard recently collected some funding, a $10 million seed round, really exciting, but that opens some questions about what does that mean for Langeanne? How does that become  How does it go from where it is right now, which is this open source startling community effort, how does it become a commercial offering? What are the plans around bouncing things? Are we talking about a freemium model? Are we talking about hosting? What kinds of things are you planning? This is a question from Vincent.  Yeah, I mean, the honest answer is we don't really know. You know, LinkedIn started off as a side project, as an open source Python package with the goal of making it easy for people to develop language model applications and that's still the primary goal today. And, you know, within within that goal of making it easy to develop there's some stuff  that 100% deserves to be open source and there's some stuff that probably will be some type of paid thing. We raised this money like two months ago or we've been exclusively focused on the open source since then. So really nothing's changed. And I think that's because as we've heard throughout this pandemic,  it's so early in the space, so much as moving. We're not focused on commercialization, we're focused on helping people build applications with language models and the best way to do that right now is by developing the open source package and that's kind of how we're thinking about it.  grand schemes or grand plans, we honestly don't know but we'll figure it out along the left. Got it. Great, you know, I feel like San Francisco's back, you know, we got, we got millions of dollars of funding, we don't have a plan but we know we got faith in the first community, we got faith in the tech. We'll do it.  Yeah, that's great to hear. All right. So that's all the time that we have. Thanks, everybody, for coming for great questions from the audience. Sorry, we couldn't answer all of them. They were just, they were probably like 50 questions in there, tried to answer the main ones. But yeah, thanks. Thanks, everyone, for coming and we'll see you all around on Twitter.  in the AI community, look forward to seeing the cool stuff everyone here builds. Awesome. Thank you everyone for joining and thank you.